{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99864a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d093f372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     c:\\Users\\anton\\Downloads\\keras_demo\\.venv\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     c:\\Users\\anton\\Downloads\\keras_demo\\.venv\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     c:\\Users\\anton\\Downloads\\keras_demo\\.venv\\nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     c:\\Users\\anton\\Downloads\\keras_demo\\.venv\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Obtenir le chemin absolu du répertoire de données NLTK\n",
    "nltk_data_dir = os.path.abspath('./.venv/nltk_data')\n",
    "\n",
    "# Définir la variable d'environnement NLTK_DATA\n",
    "os.environ['NLTK_DATA'] = nltk_data_dir\n",
    "\n",
    "# Télécharger les ressources NLTK\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "nltk.download('stopwords', download_dir=nltk_data_dir)\n",
    "nltk.download('wordnet', download_dir=nltk_data_dir)\n",
    "nltk.download('punkt_tab', download_dir=nltk_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a0e77",
   "metadata": {},
   "source": [
    "#### tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af950c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Demain', 'je', 'montrerai', 'que', \"l'intelligence\", 'artificielle', \"c'est\", 'génial', '!', '10/10']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Demain je montrerai que l'intelligence artificielle c'est génial! 10/10\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32802748",
   "metadata": {},
   "source": [
    "#### reg ex pour cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fddafec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['demain',\n",
       " 'je',\n",
       " 'montrerai',\n",
       " 'que',\n",
       " 'l',\n",
       " 'intelligence',\n",
       " 'artificielle',\n",
       " 'c',\n",
       " 'est',\n",
       " 'génial',\n",
       " '10',\n",
       " '10']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import regex\n",
    "def clean_text(text):\n",
    "    # text = re.sub(r\"[^a-zA-Z0-9]+\",' ', text)\n",
    "    text = regex.sub(r\"[^\\p{L}0-9]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.lower()\n",
    "\n",
    "clean_txt = clean_text(text)\n",
    "clean_txt\n",
    "\n",
    "tokens = word_tokenize(clean_txt)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca69950a",
   "metadata": {},
   "source": [
    "#### stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "921036ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['demain', 'montrerai', 'intelligence', 'artificielle', 'génial', '10', '10']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('french'))\n",
    "filtered = [w for w in tokens if w.lower() not in stop_words and w.isalnum()]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca77d3d",
   "metadata": {},
   "source": [
    "#### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9f5c3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['demain', 'montrerai', 'intellig', 'artificiel', 'génial', '10', '10']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stems = [stemmer.stem(m) for m in filtered]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1963fac",
   "metadata": {},
   "source": [
    "#### lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6037308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['demain', 'montrerai', 'intelligence', 'artificielle', 'génial', '10', '10']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "lemmatized = [lemmatiser.lemmatize(m) for m in filtered]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c55828b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'careful', 'carefully', 'caring', 'care', 'carol']\n"
     ]
    }
   ],
   "source": [
    "mots = [\"cars\", \"careful\", \"carefully\", \"caring\", \"care\", \"carol\"]\n",
    "lemmatized = [lemmatiser.lemmatize(m) for m in mots]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "443047e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'care', 'care', 'care', 'care', 'carol']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "stems = [stemmer.stem(m) for m in mots]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8afda9",
   "metadata": {},
   "source": [
    "#### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fd6a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   chat  chien  la  le  mange  os  poisson  regarde  souris  un\n",
      "0     1      0   1   1      1   0        0        0       1   0\n",
      "1     0      1   0   1      1   1        0        0       0   1\n",
      "2     1      0   0   2      0   0        1        1       0   0\n",
      "3     1      1   0   2      0   0        0        1       0   0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import pandas as pd \n",
    "\n",
    "mon_text = [\n",
    "    \"le chat mange la souris\",\n",
    "    \"le chien mange un os\", \n",
    "    \"le poisson regarde le chat\",\n",
    "    \"le chien regarde le chat\"\n",
    "] \n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(mon_text)\n",
    "df = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f00a1efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       chat     chien        la        le     mange        os   poisson  \\\n",
      "0  0.351295  0.000000  0.550372  0.287207  0.433919  0.000000  0.000000   \n",
      "1  0.000000  0.420493  0.000000  0.278320  0.420493  0.533343  0.000000   \n",
      "2  0.361459  0.000000  0.000000  0.591032  0.000000  0.000000  0.566295   \n",
      "3  0.385612  0.476308  0.000000  0.630527  0.000000  0.000000  0.000000   \n",
      "\n",
      "    regarde    souris        un  \n",
      "0  0.000000  0.550372  0.000000  \n",
      "1  0.000000  0.000000  0.533343  \n",
      "2  0.446473  0.000000  0.000000  \n",
      "3  0.476308  0.000000  0.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "bow = vectorizer.fit_transform(mon_text)\n",
    "df = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "114e39a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groupe 0 : ce film est super nulle\n",
      "Groupe 0 : mon telephone est cassé\n",
      "Groupe 0 : la fibre c'est génial\n",
      "Groupe 1 : le scénario est plat et les acteurs sont mauvais\n",
      "Groupe 0 : la ram ca coute très cher\n",
      "Groupe 1 : le processeur est rapide\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "mon_text=[\"ce film est super nulle\",\n",
    "          \"mon telephone est cassé\",\n",
    "          \"la fibre c'est génial\",\n",
    "          \"le scénario est plat et les acteurs sont mauvais\",\n",
    "          \"la ram ca coute très cher\",\n",
    "          \"le processeur est rapide\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(mon_text)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "for i, label in enumerate(kmeans.labels_):\n",
    "    print(f\"Groupe {label} : {mon_text[i]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
