{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "360f5c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8150b48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/lionel/DEVIA/NLP_/.venv/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lionel/DEVIA/NLP_/.venv/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/lionel/DEVIA/NLP_/.venv/nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/lionel/DEVIA/NLP_/.venv/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtenir le chemin absolu du répertoire de données NLTK\n",
    "nltk_data_dir = os.path.abspath('./.venv/nltk_data')\n",
    "\n",
    "# Définir la variable d'environnement NLTK_DATA\n",
    "os.environ['NLTK_DATA'] = nltk_data_dir\n",
    "\n",
    "# Télécharger les ressources NLTK\n",
    "nltk.download('punkt', download_dir=nltk_data_dir)\n",
    "nltk.download('stopwords', download_dir=nltk_data_dir)\n",
    "nltk.download('wordnet', download_dir=nltk_data_dir)\n",
    "nltk.download('punkt_tab', download_dir=nltk_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21132683",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a81622d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"L'intelligence\", 'artificielle', 'est', 'une', 'science', 'qui', 'permet', 'aux', 'ordinateurs', 'de', 'comprendre', 'et', 'de', 'traiter', 'le', 'langage', 'humain', '.', 'Elle', 'est', 'utilisée', 'dans', 'de', 'nombreux', 'domaines', ',', 'tels', 'que', 'la', 'reconnaissance', 'vocale', ',', 'la', 'traduction', 'automatique', 'et', 'les', 'chatbots', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"L'intelligence artificielle est une science qui permet aux ordinateurs de comprendre et de traiter le langage humain. Elle est utilisée dans de nombreux domaines, tels que la reconnaissance vocale, la traduction automatique et les chatbots.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d466b8f",
   "metadata": {},
   "source": [
    "## Nettoyage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb5c9039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l intelligence artificielle est une science qui permet aux ordinateurs de comprendre et de traiter le langage humain elle est utilisée dans de nombreux domaines tels que la reconnaissance vocale la traduction automatique et les chatbots\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import regex\n",
    "\n",
    "def clean_text(text):\n",
    "    # Supprimer les caractères spéciaux\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    # Supprimer les espaces multiples et le début et la fin de la chaîne\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.lower()\n",
    "\n",
    "def clean_text2(text):\n",
    "    # Supprimer les caractères spéciaux\n",
    "    text = regex.sub(r\"[^\\p{L}]\", \" \", text)\n",
    "    # Supprimer les espaces multiples et le début et la fin de la chaîne\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "clean_txt = clean_text2(text)\n",
    "print(clean_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57229352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['l', 'intelligence', 'artificielle', 'est', 'une', 'science', 'qui', 'permet', 'aux', 'ordinateurs', 'de', 'comprendre', 'et', 'de', 'traiter', 'le', 'langage', 'humain', 'elle', 'est', 'utilisée', 'dans', 'de', 'nombreux', 'domaines', 'tels', 'que', 'la', 'reconnaissance', 'vocale', 'la', 'traduction', 'automatique', 'et', 'les', 'chatbots']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(clean_txt)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c850d",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fec13c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['intelligence', 'artificielle', 'science', 'permet', 'ordinateurs', 'comprendre', 'traiter', 'langage', 'humain', 'utilisée', 'nombreux', 'domaines', 'tels', 'reconnaissance', 'vocale', 'traduction', 'automatique', 'chatbots']\n",
      "18\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "# Filtrer les mots non stop words\n",
    "filtered_tokens = [w for w in tokens if w.lower() not in stop_words and w.isalnum()]\n",
    "print(filtered_tokens)\n",
    "print(len(filtered_tokens))\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e100703",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201d9c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['intellig', 'artificiel', 'scienc', 'permet', 'ordinateur', 'comprendr', 'traiter', 'langag', 'humain', 'utilisé', 'nombreux', 'domain', 'tel', 'reconnaiss', 'vocal', 'traduct', 'automatiqu', 'chatbot']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stems = [stemmer.stem(w) for w in filtered_tokens]\n",
    "print(stems)\n",
    "print(len(stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfd30a1",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6031d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['intelligence', 'artificielle', 'science', 'permet', 'ordinateurs', 'comprendre', 'traiter', 'langage', 'humain', 'utilisée', 'nombreux', 'domaines', 'tels', 'reconnaissance', 'vocale', 'traduction', 'automatique', 'chatbots']\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
    "print(lemmas)\n",
    "print(len(lemmas))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb32b6",
   "metadata": {},
   "source": [
    "## Test en Anglais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa2b2ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificial', 'intelligence', 'field', 'study', 'focus', 'creating', 'intelligent', 'machine']\n"
     ]
    }
   ],
   "source": [
    "eng_text = \"Artificial intelligence is a field of study that focuses on creating intelligent machines.\"\n",
    "clean_txt2 = clean_text2(eng_text)\n",
    "tokens = word_tokenize(clean_txt2)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [w for w in tokens if w.lower() not in stop_words and w.isalnum()]\n",
    "lemmas = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3543371a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   avec  balle  canapé  chat  chien  dort  développeur  fait  je  joue  ...  \\\n",
      "0     0      0       0     0      0     0            1     0   1     0  ...   \n",
      "1     0      0       0     0      1     0            0     0   0     0  ...   \n",
      "2     0      0       0     0      0     0            0     0   0     0  ...   \n",
      "3     0      0       0     1      1     0            0     0   0     0  ...   \n",
      "4     0      0       1     1      0     1            0     0   0     0  ...   \n",
      "5     0      0       0     1      0     0            0     1   0     0  ...   \n",
      "6     1      1       0     1      0     0            0     0   0     1  ...   \n",
      "7     0      0       0     1      0     0            0     0   0     0  ...   \n",
      "\n",
      "   poisson  regarde  roule  suis  sur  un  une  vitesse  voiture  élevée  \n",
      "0        0        0      0     1    0   1    0        0        0       0  \n",
      "1        0        0      0     0    0   1    0        0        0       0  \n",
      "2        0        0      1     0    0   0    0        1        1       1  \n",
      "3        0        1      0     0    0   0    0        0        0       0  \n",
      "4        0        0      0     0    1   0    0        0        0       0  \n",
      "5        0        0      0     0    0   0    1        0        0       0  \n",
      "6        0        0      0     0    0   1    0        0        0       0  \n",
      "7        1        1      0     0    0   0    0        0        0       0  \n",
      "\n",
      "[8 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "mon_text = [\n",
    "    \"Je suis un développeur\",\n",
    "    \"Le Chien mange un os\",\n",
    "    \"La voiture roule à vitesse élevée\",\n",
    "    \"le chien regarde le chat\",\n",
    "    \"Le chat dort sur le canapé\",\n",
    "    \"Le chat fait une pause\",\n",
    "    \"Le chat joue avec un balle\",\n",
    "    \"le poisson regarde le chat\"\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(mon_text)\n",
    "df = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ee2b9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       avec     balle    canapé      chat     chien      dort  développeur  \\\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000     0.532774   \n",
      "1  0.000000  0.000000  0.000000  0.000000  0.449574  0.000000     0.000000   \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000     0.000000   \n",
      "3  0.000000  0.000000  0.000000  0.340407  0.508289  0.000000     0.000000   \n",
      "4  0.000000  0.000000  0.481467  0.270233  0.000000  0.481467     0.000000   \n",
      "5  0.000000  0.000000  0.000000  0.297275  0.000000  0.000000     0.000000   \n",
      "6  0.494605  0.494605  0.000000  0.277607  0.000000  0.000000     0.000000   \n",
      "7  0.000000  0.000000  0.000000  0.323176  0.000000  0.000000     0.000000   \n",
      "\n",
      "       fait        je      joue  ...   poisson   regarde     roule      suis  \\\n",
      "0  0.000000  0.532774  0.000000  ...  0.000000  0.000000  0.000000  0.532774   \n",
      "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.447214  0.000000   \n",
      "3  0.000000  0.000000  0.000000  ...  0.000000  0.508289  0.000000  0.000000   \n",
      "4  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "5  0.529647  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "6  0.000000  0.000000  0.494605  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "7  0.000000  0.000000  0.000000  ...  0.575794  0.482560  0.000000  0.000000   \n",
      "\n",
      "        sur        un       une   vitesse   voiture    élevée  \n",
      "0  0.000000  0.385298  0.000000  0.000000  0.000000  0.000000  \n",
      "1  0.000000  0.387946  0.000000  0.000000  0.000000  0.000000  \n",
      "2  0.000000  0.000000  0.000000  0.447214  0.447214  0.447214  \n",
      "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "4  0.481467  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "5  0.000000  0.000000  0.529647  0.000000  0.000000  0.000000  \n",
      "6  0.000000  0.357694  0.000000  0.000000  0.000000  0.000000  \n",
      "7  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[8 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "mon_text = [\n",
    "    \"Je suis un développeur\",\n",
    "    \"Le Chien mange un os\",\n",
    "    \"La voiture roule à vitesse élevée\",\n",
    "    \"le chien regarde le chat\",\n",
    "    \"Le chat dort sur le canapé\",\n",
    "    \"Le chat fait une pause\",\n",
    "    \"Le chat joue avec un balle\",\n",
    "    \"le poisson regarde le chat\"\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "bow = vectorizer.fit_transform(mon_text)\n",
    "df = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a39a548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte 1: ce film est super nul - Groupe: 1\n",
      "Texte 2: mon telephone est cassé - Groupe: 0\n",
      "Texte 3:  la fibre c'est génial - Groupe: 0\n",
      "Texte 4:  le scenario est plat et les acteurs sont mauvais - Groupe: 1\n",
      "Texte 5: le film est mal fait - Groupe: 1\n",
      "Texte 6: le film est trop long - Groupe: 1\n",
      "Texte 7:  mon telephone est vieux - Groupe: 0\n",
      "Texte 8: la fibre est faible - Groupe: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "mon_text = [\n",
    "    \"ce film est super nul\",\n",
    "    \"mon telephone est cassé\",\n",
    "    \" la fibre c'est génial\", \n",
    "    \" le scenario est plat et les acteurs sont mauvais\",\n",
    "    \"le film est mal fait\",\n",
    "    \"le film est trop long\",\n",
    "    \" mon telephone est vieux\",\n",
    "    \"la fibre est faible\",\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(mon_text)\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "for i, label in enumerate(kmeans.labels_):\n",
    "    print(f\"Texte {i+1}: {mon_text[i]} - Groupe: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493b5e0",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b12c589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "242edfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roi - homme + femme : \"kings\", cosine similarity : 0.4295138418674469\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar(positive = [\"king\"], negative = [\"man\"], topn=1)\n",
    "print(f'Roi - homme + femme : \"{result[0][0]}\", cosine similarity : {result[0][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02503f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l'intrus est : orange\n"
     ]
    }
   ],
   "source": [
    "list = [\"banana\", \"orange\", \"apple\" ,\"strawberry\" ]\n",
    "intrus = model.doesnt_match(list)\n",
    "print(f\"l'intrus est : {intrus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4ea6bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vecteur pour 'king' (Taille: 300) :\n",
      "[ 1.25976562e-01  2.97851562e-02  8.60595703e-03  1.39648438e-01\n",
      " -2.56347656e-02 -3.61328125e-02  1.11816406e-01 -1.98242188e-01\n",
      "  5.12695312e-02  3.63281250e-01 -2.42187500e-01 -3.02734375e-01\n",
      " -1.77734375e-01 -2.49023438e-02 -1.67968750e-01 -1.69921875e-01\n",
      "  3.46679688e-02  5.21850586e-03  4.63867188e-02  1.28906250e-01\n",
      "  1.36718750e-01  1.12792969e-01  5.95703125e-02  1.36718750e-01\n",
      "  1.01074219e-01 -1.76757812e-01 -2.51953125e-01  5.98144531e-02\n",
      "  3.41796875e-01 -3.11279297e-02  1.04492188e-01  6.17675781e-02\n",
      "  1.24511719e-01  4.00390625e-01 -3.22265625e-01  8.39843750e-02\n",
      "  3.90625000e-02  5.85937500e-03  7.03125000e-02  1.72851562e-01\n",
      "  1.38671875e-01 -2.31445312e-01  2.83203125e-01  1.42578125e-01\n",
      "  3.41796875e-01 -2.39257812e-02 -1.09863281e-01  3.32031250e-02\n",
      " -5.46875000e-02  1.53198242e-02 -1.62109375e-01  1.58203125e-01\n",
      " -2.59765625e-01  2.01416016e-02 -1.63085938e-01  1.35803223e-03\n",
      " -1.44531250e-01 -5.68847656e-02  4.29687500e-02 -2.46582031e-02\n",
      "  1.85546875e-01  4.47265625e-01  9.58251953e-03  1.31835938e-01\n",
      "  9.86328125e-02 -1.85546875e-01 -1.00097656e-01 -1.33789062e-01\n",
      " -1.25000000e-01  2.83203125e-01  1.23046875e-01  5.32226562e-02\n",
      " -1.77734375e-01  8.59375000e-02 -2.18505859e-02  2.05078125e-02\n",
      " -1.39648438e-01  2.51464844e-02  1.38671875e-01 -1.05468750e-01\n",
      "  1.38671875e-01  8.88671875e-02 -7.51953125e-02 -2.13623047e-02\n",
      "  1.72851562e-01  4.63867188e-02 -2.65625000e-01  8.91113281e-03\n",
      "  1.49414062e-01  3.78417969e-02  2.38281250e-01 -1.24511719e-01\n",
      " -2.17773438e-01 -1.81640625e-01  2.97851562e-02  5.71289062e-02\n",
      " -2.89306641e-02  1.24511719e-02  9.66796875e-02 -2.31445312e-01\n",
      "  5.81054688e-02  6.68945312e-02  7.08007812e-02 -3.08593750e-01\n",
      " -2.14843750e-01  1.45507812e-01 -4.27734375e-01 -9.39941406e-03\n",
      "  1.54296875e-01 -7.66601562e-02  2.89062500e-01  2.77343750e-01\n",
      " -4.86373901e-04 -1.36718750e-01  3.24218750e-01 -2.46093750e-01\n",
      " -3.03649902e-03 -2.11914062e-01  1.25000000e-01  2.69531250e-01\n",
      "  2.04101562e-01  8.25195312e-02 -2.01171875e-01 -1.60156250e-01\n",
      " -3.78417969e-02 -1.20117188e-01  1.15234375e-01 -4.10156250e-02\n",
      " -3.95507812e-02 -8.98437500e-02  6.34765625e-03  2.03125000e-01\n",
      "  1.86523438e-01  2.73437500e-01  6.29882812e-02  1.41601562e-01\n",
      " -9.81445312e-02  1.38671875e-01  1.82617188e-01  1.73828125e-01\n",
      "  1.73828125e-01 -2.37304688e-01  1.78710938e-01  6.34765625e-02\n",
      "  2.36328125e-01 -2.08984375e-01  8.74023438e-02 -1.66015625e-01\n",
      " -7.91015625e-02  2.43164062e-01 -8.88671875e-02  1.26953125e-01\n",
      " -2.16796875e-01 -1.73828125e-01 -3.59375000e-01 -8.25195312e-02\n",
      " -6.49414062e-02  5.07812500e-02  1.35742188e-01 -7.47070312e-02\n",
      " -1.64062500e-01  1.15356445e-02  4.45312500e-01 -2.15820312e-01\n",
      " -1.11328125e-01 -1.92382812e-01  1.70898438e-01 -1.25000000e-01\n",
      "  2.65502930e-03  1.92382812e-01 -1.74804688e-01  1.39648438e-01\n",
      "  2.92968750e-01  1.13281250e-01  5.95703125e-02 -6.39648438e-02\n",
      "  9.96093750e-02 -2.72216797e-02  1.96533203e-02  4.27246094e-02\n",
      " -2.46093750e-01  6.39648438e-02 -2.25585938e-01 -1.68945312e-01\n",
      "  2.89916992e-03  8.20312500e-02  3.41796875e-01  4.32128906e-02\n",
      "  1.32812500e-01  1.42578125e-01  7.61718750e-02  5.98144531e-02\n",
      " -1.19140625e-01  2.74658203e-03 -6.29882812e-02 -2.72216797e-02\n",
      " -4.82177734e-03 -8.20312500e-02 -2.49023438e-02 -4.00390625e-01\n",
      " -1.06933594e-01  4.24804688e-02  7.76367188e-02 -1.16699219e-01\n",
      "  7.37304688e-02 -9.22851562e-02  1.07910156e-01  1.58203125e-01\n",
      "  4.24804688e-02  1.26953125e-01  3.61328125e-02  2.67578125e-01\n",
      " -1.01074219e-01 -3.02734375e-01 -5.76171875e-02  5.05371094e-02\n",
      "  5.26428223e-04 -2.07031250e-01 -1.38671875e-01 -8.97216797e-03\n",
      " -2.78320312e-02 -1.41601562e-01  2.07031250e-01 -1.58203125e-01\n",
      "  1.27929688e-01  1.49414062e-01 -2.24609375e-02 -8.44726562e-02\n",
      "  1.22558594e-01  2.15820312e-01 -2.13867188e-01 -3.12500000e-01\n",
      " -3.73046875e-01  4.08935547e-03  1.07421875e-01  1.06933594e-01\n",
      "  7.32421875e-02  8.97216797e-03 -3.88183594e-02 -1.29882812e-01\n",
      "  1.49414062e-01 -2.14843750e-01 -1.83868408e-03  9.91210938e-02\n",
      "  1.57226562e-01 -1.14257812e-01 -2.05078125e-01  9.91210938e-02\n",
      "  3.69140625e-01 -1.97265625e-01  3.54003906e-02  1.09375000e-01\n",
      "  1.31835938e-01  1.66992188e-01  2.35351562e-01  1.04980469e-01\n",
      " -4.96093750e-01 -1.64062500e-01 -1.56250000e-01 -5.22460938e-02\n",
      "  1.03027344e-01  2.43164062e-01 -1.88476562e-01  5.07812500e-02\n",
      " -9.37500000e-02 -6.68945312e-02  2.27050781e-02  7.61718750e-02\n",
      "  2.89062500e-01  3.10546875e-01 -5.37109375e-02  2.28515625e-01\n",
      "  2.51464844e-02  6.78710938e-02 -1.21093750e-01 -2.15820312e-01\n",
      " -2.73437500e-01 -3.07617188e-02 -3.37890625e-01  1.53320312e-01\n",
      "  2.33398438e-01 -2.08007812e-01  3.73046875e-01  8.20312500e-02\n",
      "  2.51953125e-01 -7.61718750e-02 -4.66308594e-02 -2.23388672e-02\n",
      "  2.99072266e-02 -5.93261719e-02 -4.66918945e-03 -2.44140625e-01\n",
      " -2.09960938e-01 -2.87109375e-01 -4.54101562e-02 -1.77734375e-01\n",
      " -2.79296875e-01 -8.59375000e-02  9.13085938e-02  2.51953125e-01]\n"
     ]
    }
   ],
   "source": [
    "# Afficher le vecteur complet du mot \"joy\"\n",
    "word = \"king\"\n",
    "if word in model:\n",
    "    vector = model[word]\n",
    "    print(f\"Vecteur pour '{word}' (Taille: {len(vector)}) :\")\n",
    "    print(vector)\n",
    "else:\n",
    "    print(\"Le mot n'est pas dans le dictionnaire.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0201284d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.76094574)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = model.similarity('cat','dog')\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c489a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France : \"French\", cosine similarity : 0.7000749707221985\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar(positive = [\"France\"],topn=1)\n",
    "print(f'France : \"{result[0][0]}\", cosine similarity : {result[0][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "418a0d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I : \"[('He', 0.6712614297866821), ('him', 0.6681135892868042), ('his', 0.6201768517494202), ('she', 0.612994909286499), ('himself', 0.588027834892273), ('nobody', 0.5637064576148987), ('I', 0.555713951587677), ('it', 0.5354882478713989), ('never', 0.5239652395248413), ('somebody', 0.5205153822898865), ('that', 0.5165579915046692), ('anybody', 0.5164058804512024), ('&_quotHe', 0.5146152377128601), ('saidhe', 0.5031726956367493), ('endless_bar_mitzvahs', 0.5028665661811829), ('worry_Buddy_Shadid', 0.5022481083869934), ('thathe', 0.4992711842060089), ('me', 0.498049259185791), ('She', 0.49661505222320557), (\"hadn'tI\", 0.49516546726226807)]\"\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar(positive = [\"he\"],topn=20)\n",
    "print(f'I : \"{result}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
